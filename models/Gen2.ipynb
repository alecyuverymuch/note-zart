{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05ceb0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from data.load_data import *\n",
    "from processing.utils import *\n",
    "\n",
    "#print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "seed = 2022\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6586fa2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_midi(midi_path):\n",
    "    note_items, tempo_items = read_items(midi_path)\n",
    "    note_items = quantize_items(note_items)\n",
    "    max_time = note_items[-1].end\n",
    "    chord_items = extract_chords(note_items)\n",
    "    items = chord_items + tempo_items + note_items\n",
    "    groups = group_items(items, max_time)\n",
    "    events = item2event(groups)\n",
    "    return np.array(events, dtype=object)\n",
    "\n",
    "def transform_midi(midi_paths):\n",
    "    # extract events\n",
    "    events = []\n",
    "    for path in midi_paths:\n",
    "        events.append(read_midi(path))\n",
    "    return np.asarray(events, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "374c5424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      ">> (21,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading data...\")\n",
    "midi_paths = get_all_files(dataset_name=\"MOZART_SMALL\")\n",
    "dataset = transform_midi(midi_paths=midi_paths)\n",
    "print(f\">> {dataset.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b175d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STRUCTURE BASED DATA\n",
    "event_structs = []\n",
    "\n",
    "for e in dataset:\n",
    "    event_struct = []\n",
    "\n",
    "    for i in range(len(e)-3):\n",
    "        if e[i].name == 'Bar' and i > 0:\n",
    "            bar = Event(e[i].name, None, e[i].value, None)\n",
    "            event_struct.append(tuple([e[i]]))\n",
    "        elif e[i].name == 'Position' and \\\n",
    "            e[i+1].name == 'Note Velocity' and \\\n",
    "            e[i+2].name == 'Note On' and \\\n",
    "            e[i+3].name == 'Note Duration':\n",
    "            position = Event(e[i].name, None, e[i].value, None)\n",
    "            velocity = Event(e[i+1].name, None, e[i+1].value, None)\n",
    "            pitch = Event(e[i+2].name, None, e[i+2].value, None)\n",
    "            duration = Event(e[i+3].name, None, e[i+3].value, None)\n",
    "            event_struct.append(tuple([e[i], e[i+1], e[i+2], e[i+3]]))\n",
    "        elif e[i].name == 'Position' and e[i+1].name == 'Chord':\n",
    "            position = Event(e[i].name, None, e[i].value, None)\n",
    "            chord = Event(e[i+1].name, None, e[i+1].value, None)\n",
    "            event_struct.append(tuple([e[i], e[i+1]]))\n",
    "        elif e[i].name == 'Position' and \\\n",
    "            e[i+1].name == 'Tempo Class' and \\\n",
    "            e[i+2].name == 'Tempo Value':\n",
    "            position = Event(e[i].name, None, e[i].value, None)\n",
    "            t_class = Event(e[i+1].name, None, e[i+1].value, None)\n",
    "            t_value = Event(e[i+2].name, None, e[i+2].value, None)\n",
    "            event_struct.append(tuple([e[i], e[i+1], e[i+2]]))\n",
    "\n",
    "    event_structs.append(np.asarray(event_struct, dtype=object))\n",
    "\n",
    "event_structs = np.asarray(event_structs, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2091950b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Event Structures: 60750\n",
      "Unique Event Structures: 55983\n"
     ]
    }
   ],
   "source": [
    "all_event_structs = np.asarray(np.concatenate(event_structs), dtype=object).flat\n",
    "print(f\"All Event Structures: {len(all_event_structs)}\")\n",
    "\n",
    "_, indices = np.unique([s for s in all_event_structs], return_index=True)\n",
    "unique_event_structs = np.asanyarray([all_event_structs[i] for i in indices], dtype=object)\n",
    "print(f\"Unique Event Structures: {len(unique_event_structs)}\")\n",
    "\n",
    "struct2int = dict(zip(unique_event_structs, list(range(0, len(unique_event_structs)))))\n",
    "int2struct = {i: e for e, i in struct2int.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "41cbf165",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequenceLength = 32\n",
    "\n",
    "train_structs = []\n",
    "\n",
    "target_structs = []\n",
    "for i in range(len(event_structs)):\n",
    "    struct_list = [struct2int[s] for s in event_structs[i]]\n",
    "    for i in range(len(struct_list) - sequenceLength):\n",
    "        train_structs.append(struct_list[i:i+sequenceLength])\n",
    "        target_structs.append(struct_list[i+1])\n",
    "\n",
    "train_structs = np.asarray(train_structs)\n",
    "target_structs = np.asarray(target_structs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e14623e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60078, 32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_samples = train_structs.shape[0]\n",
    "n_structs = train_structs.shape[1]\n",
    "\n",
    "inputDim = n_structs * sequenceLength\n",
    "\n",
    "train_structs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5123b793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET BASED DATA\n",
    "\n",
    "# Define input layers\n",
    "struct_input = tf.keras.layers.Input(shape = train_structs.shape)\n",
    "\n",
    "# Define LSTM layer\n",
    "lstm_layer = tf.keras.layers.LSTM(512, return_sequences=True)(struct_input)\n",
    "\n",
    "# Define dense layer\n",
    "dense_layer = tf.keras.layers.Dense(256)(lstm_layer)\n",
    "\n",
    "# Define output layers\n",
    "set_output = tf.keras.layers.Dense(n_structs, activation = 'softmax')(dense_layer)\n",
    "\n",
    "# Define model\n",
    "lstm = tf.keras.Model(inputs = struct_input, outputs = set_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0dd2ac3d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected input_3 to have 3 dimensions, but got array with shape (60078, 32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-baed36fd2a79>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mlstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_structs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_structs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m           distribution_strategy=strategy)\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data_adapter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_training_inputs\u001b[0;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, steps_per_epoch, validation_split, validation_data, validation_steps, shuffle, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    545\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    548\u001b[0m     \u001b[0mval_adapter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[0;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m         \u001b[0mcheck_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m         steps=steps)\n\u001b[0m\u001b[1;32m    595\u001b[0m   adapter = adapter_cls(\n\u001b[1;32m    596\u001b[0m       \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2470\u001b[0m           \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2471\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2472\u001b[0;31m           exception_prefix='input')\n\u001b[0m\u001b[1;32m   2473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m     \u001b[0;31m# Get typespecs for the input data and sanitize it if necessary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    563\u001b[0m                            \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m                            \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 565\u001b[0;31m                            'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    566\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m           \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected input_3 to have 3 dimensions, but got array with shape (60078, 32)"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "lstm.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "# Train the model\n",
    "lstm.fit(train_structs, target_structs, epochs=500, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ecf5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initial_notes = np.expand_dims(train_notes[0,:].copy(), 0)\n",
    "#initial_tempos = np.expand_dims(train_tempo[0,:].copy(), 0)\n",
    "\n",
    "#def predictChords(note_sequence, tempo_sequence):\n",
    "#    predicted_notes, predicted_tempo = lstm.predict(lstm.predict([note_sequence, tempo_sequence]))\n",
    "#    return np.argmax(predicted_notes), np.argmax(predicted_tempo)\n",
    "\n",
    "# Define empty lists for generated chords and durations\n",
    "#new_notes, new_tempos = [], []\n",
    "\n",
    "# Generate chords and durations using 500 rounds of prediction\n",
    "#for j in range(500):\n",
    "#    new_note, new_tempo = predictChords(initial_notes, initial_tempos)\n",
    "#    new_notes.append(new_note)\n",
    "#    new_tempos.append(new_tempo)\n",
    "#    initial_notes[0][:-1] = initial_notes[0][1:]\n",
    "#    initial_notes[0][-1] = new_note\n",
    "#    initial_tempos[0][:-1] = initial_tempos[0][1:]\n",
    "#    initial_tempos[0][-1] = new_tempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ecf5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_sets = np.expand_dims(train_sets[0,:].copy(), 0)\n",
    "\n",
    "def predictChords(set_sequence):\n",
    "    predicted_sets= lstm.predict(set_sequence)\n",
    "    return np.argmax(predicted_sets)\n",
    "\n",
    "# Define empty lists for generated chords and durations\n",
    "new_sets = []\n",
    "\n",
    "# Generate chords and durations using 500 rounds of prediction\n",
    "for j in range(500):\n",
    "    new_set = predictChords(initial_sets)\n",
    "    new_sets.append(new_set)\n",
    "    initial_sets[0][:-1] = initial_sets[0][1:]\n",
    "    initial_sets[0][-1] = new_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac923e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sets[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16e33a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_events = np.asarray(sum(new_sets,()), dtype=object)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
